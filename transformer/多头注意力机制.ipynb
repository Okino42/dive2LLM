{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "\n",
    "**1\\. 多头注意力的基本流程**\n",
    "------------------\n",
    "\n",
    "### **(1) 输入和投影**\n",
    "\n",
    "假设我们有一个输入张量 $X$：\n",
    "\n",
    "*   **形状：** $(batch\\_size, seq\\_len, hidden\\_dim)$\n",
    "\n",
    "我们使用 **三个线性变换**（`nn.Linear`）得到：\n",
    "\n",
    "*   **查询（Query, Q）**\n",
    "*   **键（Key, K）**\n",
    "*   **值（Value, V）**\n",
    "\n",
    "这些变换通常共享 `hidden_dim` 作为输入和输出的维度：\n",
    "\n",
    "$$Q, K, V = X W_Q, X W_K, X W_V$$\n",
    "\n",
    "其中：\n",
    "\n",
    "*   $W_Q, W_K, W_V$ 是线性变换矩阵，形状为 $(hidden\\_dim, hidden\\_dim)$。\n",
    "\n",
    "### **(2) 切分成多个头**\n",
    "\n",
    "为了让注意力机制能够从不同的子空间学习不同的信息，我们引入 `head_num` 个头，每个头的维度是 `head_dim`，并将 `hidden_dim` 平均分成 `head_num` 份：\n",
    "\n",
    "$$head\\_dim = \\frac{hidden\\_dim}{head\\_num}$$\n",
    "\n",
    "然后，我们将 $Q, K, V$ **reshape** 为：\n",
    "\n",
    "$$(batch\\_size, seq\\_len, head\\_num, head\\_dim)$$\n",
    "\n",
    "然后进行维度转换，使其变为：\n",
    "\n",
    "$$(batch\\_size, head\\_num, seq\\_len, head\\_dim)$$\n",
    "\n",
    "这样，每个头就可以单独计算自注意力。\n",
    "\n",
    "### **(3) 计算每个头的注意力**\n",
    "\n",
    "对于每个头 $i$，计算：\n",
    "\n",
    "$$\\text{Attention}_i = \\text{softmax} \\left( \\frac{Q_i K_i^T}{\\sqrt{head\\_dim}} \\right) V_i$$\n",
    "\n",
    "其中：\n",
    "\n",
    "*   $Q_i, K_i, V_i$ 的形状均为 $(batch\\_size, seq\\_len, head\\_dim)$。\n",
    "\n",
    "### **(4) 多头结果合并**\n",
    "\n",
    "计算完所有头的注意力后，我们需要将多个头的结果拼接回原始维度：\n",
    "\n",
    "$$\\text{MultiHeadOutput} = \\text{Concat}(\\text{Attention}_1, ..., \\text{Attention}_h)$$\n",
    "\n",
    "这样拼接后，形状变为：\n",
    "\n",
    "$$(batch\\_size, seq\\_len, head\\_num \\times head\\_dim)$$\n",
    "\n",
    "为了保证最终的输出维度仍然是 `hidden_dim`，**必须满足**：\n",
    "\n",
    "$$head\\_num \\times head\\_dim = hidden\\_dim$$\n",
    "\n",
    "* * *\n",
    "\n",
    "**2\\. 为什么要保证 `head_num * head_dim = hidden_dim`？**\n",
    "--------------------------------------------------\n",
    "\n",
    "1.  **保证输入和输出的维度一致**\n",
    "    \n",
    "    *   这样 Residual Connection（残差连接）才能正确相加： \n",
    "        $$X + \\text{MultiHeadOutput}$$\n",
    "        \n",
    "    *   如果 `MultiHeadOutput` 维度不匹配 `hidden_dim`，残差连接无法进行。\n",
    "2.  **确保计算效率**\n",
    "    \n",
    "    *   这样可以直接拼接多个头的输出，而不需要额外的维度变换。\n",
    "3.  **让每个注意力头关注不同的信息**\n",
    "    \n",
    "    *   例如，某个头可能关注语义信息，另一个头可能关注位置关系。\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weight is \n",
      " tensor([[[[   -inf, -0.2542],\n",
      "          [   -inf, -0.1895]],\n",
      "\n",
      "         [[   -inf, -0.1817],\n",
      "          [   -inf, -0.2573]],\n",
      "\n",
      "         [[   -inf,  0.0769],\n",
      "          [   -inf,  0.0249]],\n",
      "\n",
      "         [[   -inf,  0.2377],\n",
      "          [   -inf,  0.1909]],\n",
      "\n",
      "         [[   -inf,  0.0548],\n",
      "          [   -inf,  0.0169]],\n",
      "\n",
      "         [[   -inf, -0.0766],\n",
      "          [   -inf, -0.1476]],\n",
      "\n",
      "         [[   -inf, -0.1451],\n",
      "          [   -inf, -0.1433]],\n",
      "\n",
      "         [[   -inf, -0.1044],\n",
      "          [   -inf, -0.3028]]],\n",
      "\n",
      "\n",
      "        [[[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1485,    -inf],\n",
      "          [-0.0692,    -inf]],\n",
      "\n",
      "         [[-0.2032,    -inf],\n",
      "          [-0.1270,    -inf]],\n",
      "\n",
      "         [[ 0.1452,    -inf],\n",
      "          [ 0.1591,    -inf]],\n",
      "\n",
      "         [[ 0.2732,    -inf],\n",
      "          [ 0.1799,    -inf]],\n",
      "\n",
      "         [[ 0.1126,    -inf],\n",
      "          [-0.1315,    -inf]],\n",
      "\n",
      "         [[-0.0880,    -inf],\n",
      "          [-0.1717,    -inf]],\n",
      "\n",
      "         [[ 0.0257,    -inf],\n",
      "          [-0.0818,    -inf]],\n",
      "\n",
      "         [[-0.2172,    -inf],\n",
      "          [-0.1055,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7794e-02, -3.8403e-02, -1.7401e-01,  1.8290e-01, -7.4299e-02,\n",
       "           1.3316e-01, -1.9407e-01, -3.7210e-01,  2.2652e-01,  9.2041e-02,\n",
       "           1.8681e-01,  7.9244e-02,  4.3188e-01,  1.9363e-01, -1.8323e-01,\n",
       "           3.4159e-01, -1.6718e-01, -5.3080e-02,  3.9149e-02,  3.1558e-02,\n",
       "           3.2350e-01, -3.9447e-01,  3.0021e-01,  2.3686e-01, -1.8960e-01,\n",
       "           2.5181e-01, -1.1322e-02, -3.7456e-01,  4.6022e-02, -1.1317e-01,\n",
       "          -7.0249e-03, -2.3937e-01, -2.7386e-01,  1.2554e-01, -1.5031e-01,\n",
       "          -1.8619e-02, -1.9030e-01,  1.6299e-01,  2.8923e-01, -1.1642e-01,\n",
       "          -8.8469e-02, -3.7217e-01, -1.5708e-01, -1.4586e-01,  3.7437e-01,\n",
       "           3.9479e-01,  9.8606e-02,  2.1099e-01, -3.7894e-03, -7.2547e-02,\n",
       "          -5.6828e-02,  1.6051e-01,  1.7420e-01, -7.9083e-02,  1.2469e-01,\n",
       "          -6.9784e-02, -4.9505e-02,  1.6026e-01,  3.1288e-01,  3.5575e-01,\n",
       "           5.4081e-02, -2.1109e-01, -3.7861e-01, -3.0217e-01, -2.5117e-01,\n",
       "           2.2149e-01,  3.1825e-01,  7.4785e-02,  3.7631e-02, -1.5357e-02,\n",
       "           4.3713e-02,  5.6154e-02, -1.5707e-01, -2.9675e-01, -5.7628e-02,\n",
       "           9.2630e-02,  2.0086e-01,  1.4851e-01,  9.6232e-02, -1.8098e-01,\n",
       "           3.0867e-01, -1.0443e-01, -4.6139e-02, -3.4163e-01, -1.7050e-02,\n",
       "           6.7587e-02, -1.9838e-01, -5.1823e-02, -9.5919e-02, -2.3893e-01,\n",
       "          -1.0971e-01,  4.8495e-01,  1.4671e-01, -2.6112e-01,  1.8972e-01,\n",
       "          -1.3063e-01, -2.9472e-01,  2.2918e-02,  6.1962e-02, -1.2042e-01,\n",
       "          -3.2700e-01,  1.9859e-01, -1.2881e-01,  6.8662e-02, -1.9668e-01,\n",
       "           1.2474e-01, -2.7012e-01, -8.3169e-02, -7.9995e-02,  4.4786e-02,\n",
       "          -7.4984e-02,  8.9581e-02, -5.3679e-02,  1.3017e-01, -4.3619e-01,\n",
       "          -7.9119e-02, -1.8286e-01,  1.7536e-01, -3.0222e-01, -8.3689e-02,\n",
       "          -1.2293e-01, -3.5868e-01, -7.1774e-03, -1.0240e-01,  1.6712e-01,\n",
       "           4.5817e-02, -1.8956e-02,  3.9206e-01],\n",
       "         [ 2.2534e-01,  6.8728e-02,  1.2128e-01,  5.3025e-04, -5.5350e-02,\n",
       "           1.4338e-01, -1.6767e-01, -2.8107e-01,  3.1634e-01,  1.8651e-01,\n",
       "           3.5385e-02,  1.1052e-01,  2.0539e-01,  2.2280e-01, -2.2826e-01,\n",
       "           1.6623e-01, -2.7815e-01, -1.1182e-01,  2.3111e-01, -9.2584e-03,\n",
       "           2.5834e-01, -1.9309e-01,  2.6843e-01,  1.5723e-01, -6.3179e-02,\n",
       "           1.1918e-01,  1.7489e-01, -2.6723e-01,  5.6161e-02, -1.9686e-01,\n",
       "           2.3155e-01, -2.1112e-01, -1.0048e-01,  1.0063e-01, -3.2298e-01,\n",
       "          -2.9737e-01, -4.8953e-03,  2.4341e-01,  2.5371e-01, -2.4714e-01,\n",
       "           1.8477e-01, -2.8891e-01, -2.7841e-01, -4.1744e-01,  3.7645e-01,\n",
       "           4.3363e-01, -2.6038e-02, -4.9982e-02,  1.7016e-01, -9.5972e-02,\n",
       "          -3.6941e-03,  2.6802e-01,  5.0307e-02, -1.1782e-01,  2.0851e-01,\n",
       "          -2.3962e-02, -1.2970e-01,  1.6698e-01,  2.6855e-01,  2.5983e-01,\n",
       "           2.6547e-01, -2.7615e-01, -3.8385e-01, -5.1939e-01, -7.3115e-02,\n",
       "           1.9997e-01,  2.0037e-01, -1.2913e-01,  6.4278e-02,  5.0035e-02,\n",
       "           2.1348e-01, -1.2780e-02, -3.0283e-01,  9.2361e-02, -1.4614e-01,\n",
       "           1.9866e-01,  3.5217e-01,  2.7203e-01, -3.5194e-03,  1.7380e-01,\n",
       "           1.7799e-01,  2.4055e-02, -3.4943e-01, -2.2274e-01, -9.5114e-03,\n",
       "           5.4730e-03,  8.0582e-02, -1.0996e-01,  8.2654e-02, -4.7934e-02,\n",
       "           7.9179e-03,  5.1590e-01,  1.8344e-01, -3.7286e-01,  1.1139e-01,\n",
       "          -1.8290e-01, -3.6983e-01,  1.0814e-01,  4.2589e-02, -9.4723e-02,\n",
       "          -2.7697e-01,  4.2553e-02, -2.5093e-01, -3.7066e-03, -2.6556e-01,\n",
       "           9.1825e-02,  3.5171e-02,  2.8576e-01,  5.8924e-03, -2.1782e-02,\n",
       "          -7.1621e-02,  1.4012e-01, -1.3499e-01, -1.3429e-01, -1.7641e-01,\n",
       "          -1.4254e-01, -2.2271e-01,  2.4996e-01, -3.4219e-01,  1.0106e-01,\n",
       "           1.1913e-01, -6.6053e-02, -1.9229e-02, -1.8013e-01,  2.6725e-01,\n",
       "          -1.3511e-02,  2.9853e-02,  2.7823e-01]],\n",
       "\n",
       "        [[        nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan],\n",
       "         [        nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan]],\n",
       "\n",
       "        [[ 2.3654e-01, -3.2776e-01,  1.0373e-01,  2.6965e-02, -1.8533e-01,\n",
       "           3.0368e-01, -1.2970e-01, -3.6218e-01,  1.1397e-01,  5.7110e-01,\n",
       "           2.9551e-02,  2.4221e-01,  3.6074e-01,  2.4479e-01, -1.9154e-01,\n",
       "           1.7057e-01, -1.9332e-01, -1.0630e-01,  1.6823e-01,  1.6099e-01,\n",
       "           2.0471e-01, -2.0686e-01,  3.3666e-01,  5.6716e-01, -2.8614e-01,\n",
       "          -8.1685e-02, -8.6526e-02, -4.1153e-01, -2.8623e-01, -2.0927e-01,\n",
       "          -2.5734e-04, -1.0199e-01, -1.5458e-01, -2.9557e-01, -8.3823e-02,\n",
       "           7.1834e-02, -1.6570e-01,  1.2241e-01,  4.7252e-01, -6.1293e-02,\n",
       "           1.4430e-01, -7.9188e-02, -3.1284e-01, -6.4018e-01,  4.9393e-01,\n",
       "           4.3219e-01, -1.3406e-01,  1.5254e-01,  2.0586e-01, -5.5282e-02,\n",
       "          -5.3315e-01, -1.0343e-01,  2.9052e-01,  7.5734e-02,  2.7355e-01,\n",
       "          -1.7451e-01, -2.1519e-01,  2.6268e-01,  3.7785e-01,  4.2931e-01,\n",
       "           1.3567e-01, -2.3222e-01, -4.5452e-01, -3.3918e-01, -1.7631e-01,\n",
       "          -8.9280e-02,  1.7149e-01,  1.6357e-01, -4.5013e-02,  2.8249e-01,\n",
       "           1.1844e-03, -5.7279e-02, -5.1299e-01, -1.0761e-01, -6.4827e-02,\n",
       "           1.6217e-01,  2.5539e-01,  1.6225e-01, -5.2276e-02, -1.1496e-01,\n",
       "           2.7104e-02, -6.4273e-02,  3.4458e-02, -6.5825e-01,  3.4680e-03,\n",
       "           1.7529e-01, -3.7179e-01, -2.2333e-01,  1.4830e-01,  9.5535e-02,\n",
       "          -3.9184e-01,  3.8970e-01,  2.9474e-01, -1.2750e-02,  1.2032e-01,\n",
       "          -6.8267e-02,  2.1949e-01, -1.5383e-02, -1.4120e-01,  7.4151e-02,\n",
       "          -3.8387e-01,  1.5919e-01,  1.3982e-01,  2.3517e-01,  8.5675e-02,\n",
       "          -5.2111e-02, -8.1879e-02,  2.2627e-01,  1.0619e-01,  4.0283e-01,\n",
       "           1.0514e-01,  2.8261e-01,  3.6894e-03,  1.0283e-01, -4.9827e-01,\n",
       "          -2.2171e-01, -2.2736e-01,  2.1250e-01, -4.1512e-02,  2.0830e-01,\n",
       "          -1.2816e-01,  1.1457e-02,  1.7418e-02, -2.4105e-01,  2.8847e-01,\n",
       "           2.9684e-01, -2.4550e-01,  9.2106e-01],\n",
       "         [ 2.3654e-01, -3.2776e-01,  1.0373e-01,  2.6965e-02, -1.8533e-01,\n",
       "           3.0368e-01, -1.2970e-01, -3.6218e-01,  1.1397e-01,  5.7110e-01,\n",
       "           2.9551e-02,  2.4221e-01,  3.6074e-01,  2.4479e-01, -1.9154e-01,\n",
       "           1.7057e-01, -1.9332e-01, -1.0630e-01,  1.6823e-01,  1.6099e-01,\n",
       "           2.0471e-01, -2.0686e-01,  3.3666e-01,  5.6716e-01, -2.8614e-01,\n",
       "          -8.1685e-02, -8.6526e-02, -4.1153e-01, -2.8623e-01, -2.0927e-01,\n",
       "          -2.5734e-04, -1.0199e-01, -1.5458e-01, -2.9557e-01, -8.3823e-02,\n",
       "           7.1834e-02, -1.6570e-01,  1.2241e-01,  4.7252e-01, -6.1293e-02,\n",
       "           1.4430e-01, -7.9188e-02, -3.1284e-01, -6.4018e-01,  4.9393e-01,\n",
       "           4.3219e-01, -1.3406e-01,  1.5254e-01,  2.0586e-01, -5.5282e-02,\n",
       "          -5.3315e-01, -1.0343e-01,  2.9052e-01,  7.5734e-02,  2.7355e-01,\n",
       "          -1.7451e-01, -2.1519e-01,  2.6268e-01,  3.7785e-01,  4.2931e-01,\n",
       "           1.3567e-01, -2.3222e-01, -4.5452e-01, -3.3918e-01, -1.7631e-01,\n",
       "          -8.9280e-02,  1.7149e-01,  1.6357e-01, -4.5013e-02,  2.8249e-01,\n",
       "           1.1844e-03, -5.7279e-02, -5.1299e-01, -1.0761e-01, -6.4827e-02,\n",
       "           1.6217e-01,  2.5539e-01,  1.6225e-01, -5.2276e-02, -1.1496e-01,\n",
       "           2.7104e-02, -6.4273e-02,  3.4458e-02, -6.5825e-01,  3.4680e-03,\n",
       "           1.7529e-01, -3.7179e-01, -2.2333e-01,  1.4830e-01,  9.5535e-02,\n",
       "          -3.9184e-01,  3.8970e-01,  2.9474e-01, -1.2750e-02,  1.2032e-01,\n",
       "          -6.8267e-02,  2.1949e-01, -1.5383e-02, -1.4120e-01,  7.4151e-02,\n",
       "          -3.8387e-01,  1.5919e-01,  1.3982e-01,  2.3517e-01,  8.5675e-02,\n",
       "          -5.2111e-02, -8.1879e-02,  2.2627e-01,  1.0619e-01,  4.0283e-01,\n",
       "           1.0514e-01,  2.8261e-01,  3.6894e-03,  1.0283e-01, -4.9827e-01,\n",
       "          -2.2171e-01, -2.2736e-01,  2.1250e-01, -4.1512e-02,  2.0830e-01,\n",
       "          -1.2816e-01,  1.1457e-02,  1.7418e-02, -2.4105e-01,  2.8847e-01,\n",
       "           2.9684e-01, -2.4550e-01,  9.2106e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MutiHeadSelfAttentionFormal(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, attention_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim  = hidden_dim // head_num #head_num * head_dim = hidden_dim\n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "    \n",
    "    def forward(self,X,attention_mask=None):\n",
    "        #X shape (batch,seq,hidden_dim)\n",
    "        batch,seq_len,_ =X.size()\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        v = self.v_proj(X) #shape(b,seq,hidden_dim)\n",
    "        #(b,seq,hidden_dim) --> (b,head_num,seq,head_dim)\n",
    "        #因此需要将hidden_dim拆成head_dim*head_num\n",
    "        q_state = Q.view(batch,seq_len,self.head_num,self.head_dim).transpose(1,2)\n",
    "        k_state = K.view(batch,seq_len,self.head_num,self.head_dim).transpose(1,2)\n",
    "        v_state = v.view(batch,seq_len,self.head_num,self.head_dim).transpose(1,2)\n",
    "        #attention_weight shape (b,head_num,s,s)\n",
    "        attention_weight = torch.matmul( \n",
    "            q_state,k_state.transpose(-1,-2)  #(b,head_num,seq,head_dim)  --> (b,head_num,head_dim,seq)\n",
    "        ) / torch.sqrt(torch.tensor(self.head_dim,device=X.device))\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        print(f\"attention_weight is \\n {attention_weight}\")\n",
    "        attention_weight = torch.softmax(attention_weight,dim=-1)\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        output_mid = torch.matmul(attention_weight,v_state) #(b,head_num,seq,head_dim)\n",
    "        output_mid = output_mid.transpose(1,2).contiguous()\n",
    "        output_mid = output_mid.view(batch,seq_len,-1) #-1自动填充为hidden_dim维度\n",
    "        output = self.out_proj(output_mid)\n",
    "        return output\n",
    "    \n",
    "\n",
    "attention_mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0,1],\n",
    "            [0,0],\n",
    "            [1,0]\n",
    "        ]\n",
    "    ).unsqueeze(1).unsqueeze(2).expand(3,8,2,2)\n",
    ")\n",
    "\n",
    "X = torch.rand(3,2,128)\n",
    "net  = MutiHeadSelfAttentionFormal(128,8) #head_dim = 16\n",
    "net(X,attention_mask)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
