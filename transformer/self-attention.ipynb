### 第一重境界 简化版本
import torch
import torch.nn as nn


class SelfAttentionV1(nn.Module):
    def __init__(self,hidden_dim:int = 728) -> None:
        super().__init__()
        self.hidden_dim = hidden_dim
        #初始化不同的三个线性映射层
        #nn.Linear(in_features, out_features) 是 PyTorch 中用来创建一个 全连接层（Linear layer） 的方法。这个层会进行一个线性变换,将输入的每个元素的特征维度进行转化
        # nn.Linear中的W和b是由 PyTorch 自动初始化的，权重是从一个符合 均匀分布 或 正态分布 的随机数中初始化的，偏置项默认会被初始化为 零
        #保持输入输出维度一致：1.简化计算：在某些场景下，保持维度一致能简化模型的计算，避免额外的维度转换。2.残差连接
        self.query_proj = nn.Linear(hidden_dim,hidden_dim)  #proj 是 projection（投影）的缩写
        self.key_proj = nn.Linear(hidden_dim,hidden_dim)
        self.value_proj = nn.Linear(hidden_dim,hidden_dim)
        #必须写 self，否则你定义的将是一个局部变量，它不能在类的其他方法中访问。self 确保了变量是类的实例属性，能够在类的所有方法中共享和使用。

    
    def forward(self,X):
        # X shape is : (batch_size,seq_len,hidden_dim)
        #初始化QKV3个张量
        Q = self.query_proj(X)
        #nn.Linear(hidden_dim, hidden_dim) 会对 X 中的每个 hidden_dim 特征的向量进行变换，而不会影响 batch_size 和 seq_len 维度。结果会得到一个形状为 (batch_size, seq_len, hidden_dim) 的输出。
        K = self.key_proj(X)
        V = self.value_proj(X)
        # Q K V shape (batch,seq_len,hidden_dim)

        # attention_value shape (batch_size,seq_len,seq_len)
        #批量矩阵乘法中的 batch_size 必须一致，即同一批次的矩阵对需要具有相同的批次大小。
        attention_value = torch.matmul(
            # K需要变成：（batch_size,hidden_dim,seq_len)
            Q,K.transpose(-1,-2)
        )

        attention_weight = torch.softmax(
            attention_value / torch.sqrt(torch.tensor(self.hidden_dim, dtype=torch.float32)),
            dim=-1
        )
        # print(attention_weight)
        #输入和输出的shape保持一致，((batch_size,seq_len,hidden_dim))
        output = torch.matmul(attention_weight,V)
        return output

#测试
X = torch.rand(3,2,4)
# print(X)
self_att_net = SelfAttentionV1(4)
self_att_net(X) #使用__call__方法，相当于self_att_net.forward(X)

### 第二重境界 效率优化
网络比较小的时候,QKV是可以合在一起写的
class SelfAttentionV2(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.proj = nn.Linear(dim,dim*3)

    def forward(self,X):
        #x shape (batch,seq,dim)
        #QKV shape (batch,seq,dim*3)
        QKV = self.proj(X)
        print("QKV的")
        Q,K,V = torch.split(QKV,self.dim,dim=-1)
        att_weight = torch.softmax(
            torch.matmul(Q,K.transpose(-1,-2)) / torch.sqrt(torch.tensor(self.dim,dtype=torch.float32)),
            dim=-1
        )
        output = torch.matmul(att_weight,V)
        return output

X = torch.rand(3,2,4)
self_att_net2 = SelfAttentionV2(4)
self_att_net2(X)

### 第三重境界 加入一些细节
1.dropout 2.sequence的长度不一样，有mask
# 1.dropout的位置
# 2.attention_mask
# 3.output 矩阵映射（可选）

class SelfAttentionV3(nn.Module):
    def __init__(self, dim,dropout_rate=0.1,*args, **kwargs):
        super().__init__(*args, **kwargs)
        self.dim = dim
        self.proj = nn.Linear(dim,dim*3)
        self.attention_dropout = nn.Dropout(dropout_rate)

        #可选
        self.output_proj = nn.Linear(dim,dim)

    def forward(self,X,attention_mask=None):
        # X shape (batch,seq,dim)
        QKV = self.proj(X)
        Q,K,V = torch.split(QKV,self.dim,dim=-1)
        #attention_weight shape (batch,seq,seq)
        attention_weight = torch.matmul(Q,K.transpose(-1,-2)) / torch.sqrt(torch.tensor(self.dim,dtype=torch.float32))
        if attention_mask is not None:
            attention_weight = attention_weight.masked_fill(
                attention_mask == 0,
                float("-1e20")
            )
        print(f"attention_weight is:\n{attention_weight}")
        attention_weight = torch.softmax(
            attention_weight,
            dim = -1
        )
        #Dropout 层将在训练过程中对这些权重进行随机丢弃（即将一些权重设为零），以增加模型的鲁棒性
        attention_weight = self.attention_dropout(attention_weight)
        attention_result = attention_weight @ V
        output  = self.proj(attention_result)
        return output
    
X = torch.rand(3,4,2)
#目前的mark shape (batch,seq)
mark = torch.tensor(
    [
        [1,1,1,0],
        [1,1,0,0],
        [1,0,0,0]
    ]
)
print("mark shape is:",mark.shape)
#转化成mark shape (batch,seq,seq),和attention_weight shape一致，也就是（3，4，4）
print(f"mark 在1位置增加维度:\n{mark.unsqueeze(dim=1)}")
mark = mark.unsqueeze(dim=1).repeat(1,4,1)
print(f"repeat mark shape is:{mark.size()}")
print(f"repeat mark is: \n {mark}")

net = SelfAttentionV3(2)
net(X,mark)


### 第四重境界 面试写法
class SelfAttentionInterview(nn.Module):
    def __init__(self, dim:int, dropout_rate:float=0.1):
        super().__init__()
        self.dim = dim

        self.query = nn.Linear(dim,dim)
        self.key = nn.Linear(dim,dim)
        self.value = nn.Linear(dim,dim)

        self.attention_dropout = nn.Dropout(dropout_rate)

    def forward(self,X,attention_mask=None):
        #X shape (batch,seq,dim)
        Q = self.query(X)
        K = self.key(X)
        V = self.value(X)

        attention_weight = Q @ K.transpose(-1,-2) / torch.sqrt(torch.tensor(self.dim,dtype=torch.float32))
        if attention_mask is not None:
            attention_weight = attention_weight.masked_fill(
                attention_mask == 0,
                float("-inf")
            )
        #attention_weight shape (batch,seq,seq)
        attention_weight = torch.softmax(
            attention_weight,
            dim = -1
        )

        attention_weight = self.attention_dropout(attention_weight)
        #output shape (batch,seq,dim)
        output = attention_weight @ V
        return output
    

X = torch.rand(3,4,2)
mark = torch.tensor(
[
    [1,1,1,0],
    [1,1,0,0],
    [1,0,0,0]
]
)
#转化成mark shape (batch,seq,seq),和attention_weight shape一致，也就是（3，4，4）
mark = mark.unsqueeze(dim=1).repeat(1,4,1)
net  = SelfAttentionInterview(2)
net(X,attention_mask = mark)

    
